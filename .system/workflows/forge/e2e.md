# FORGE: E2E - Automated Browser Testing

## Feature Folder: $ARGUMENTS

Run automated E2E browser tests for a FORGE feature, capturing screenshots and generating a comprehensive test report.

---

## Overview

This command:
1. **Detects** if E2E testing is needed based on SPEC content
2. **Parses** success criteria to generate test cases
3. **Executes** browser tests using agent-browser
4. **Captures** screenshots at key checkpoints
5. **Generates** `E2E-REPORT.md` with results and evidence

---

## Prerequisites

Before running this command:
- [ ] `/forge:run` has been completed
- [ ] Dev environment is running (backend + frontend)
- [ ] SPEC.md exists with success criteria or manual verification section

---

## When to Run E2E Tests

### Automatic Triggers (ANY of these = run E2E)

**Content-based triggers** (check SPEC.md):
- Success criteria mention: "visible", "button", "page", "UI", "form", "click", "navigate", "display", "show"
- Has "Manual Verification" section with UI steps
- Mentions frontend pages or components

**File-based triggers** (check .forge-progress.md):
- Frontend files modified (`frontend/src/**`)
- UI components created/changed
- Routes added

**Feature-type triggers:**
- Feature involves user-facing changes
- Feature adds new pages or modals
- Feature changes navigation or layout

### Skip E2E When

- Backend-only changes (API, database, services)
- Documentation updates
- Test file changes only
- Config/environment changes

---

## Input Files

```bash
READ $ARGUMENTS/SPEC.md
READ $ARGUMENTS/.forge-progress.md  # If exists
READ $ARGUMENTS/REVIEW.md  # If exists
```

---

## Phase 1: Detection & Analysis

### Step 1.1: Check if E2E Tests Needed

Parse SPEC.md and determine if E2E testing is required:

```markdown
## E2E Detection Analysis

### Content Signals
- [ ] Success criteria mention UI elements
- [ ] Manual verification section exists
- [ ] Frontend routes referenced

### File Signals
- [ ] Frontend files in .forge-progress.md
- [ ] Component files created/modified
- [ ] Page files created/modified

### Decision
- **Run E2E:** Yes / No
- **Reason:** [Explanation]
```

If E2E tests are NOT needed, generate a minimal report and exit:

```markdown
# E2E Report: {Feature Name}

**Date:** {YYYY-MM-DD}
**Status:** ⏭️ Skipped

## Reason
E2E testing not required for this feature.

**Detection Analysis:**
- No UI changes detected
- No frontend files modified
- Success criteria are backend-only

**Files Changed:**
[List from .forge-progress.md]

---
*Generated by /forge:e2e - No browser tests executed*
```

---

## Phase 2: Test Case Generation

### Step 2.1: Parse Success Criteria

Extract testable scenarios from SPEC.md:

```markdown
## Extracted Test Cases

### From Success Criteria
| Criterion | Test Type | Priority |
|-----------|-----------|----------|
| "Phone import UI visible" | Visual verification | P0 |
| "Button shows on click" | Interaction test | P0 |
| "Form submits successfully" | Workflow test | P0 |

### From Manual Verification
| Test Name | Steps | Expected |
|-----------|-------|----------|
| [Name from SPEC] | [Steps from SPEC] | [Expected from SPEC] |
```

### Step 2.2: Generate Test Plan

Create structured test cases:

```markdown
## Test Plan

### Test Suite: {Feature Name}

**Total Tests:** {N}
**Estimated Duration:** {X} minutes

---

### TC-001: {Test Name}
**Type:** Visual / Interaction / Workflow / Error Handling
**Priority:** P0 / P1 / P2
**Requires Auth:** Yes / No

**Preconditions:**
- [List any setup required]

**Steps:**
1. Navigate to {URL}
2. [Action]
3. [Verification]

**Expected Result:**
- [What should be true]

**Screenshot Points:**
- Before action: `tc-001-before.png`
- After action: `tc-001-after.png`

---

### TC-002: {Next Test}
...
```

---

## Phase 3: Environment Setup

### Step 3.1: Verify Dev Environment

```bash
# Check frontend is running
curl -s http://localhost:5173 > /dev/null && echo "Frontend: ✓ Running" || echo "Frontend: ✗ Not running"

# Check backend is running
curl -s http://localhost:8000/health > /dev/null && echo "Backend: ✓ Running" || echo "Backend: ✗ Not running"
```

If either is not running, prompt user:

```markdown
## Environment Check Failed

The E2E tests require the dev environment to be running.

**Status:**
- Frontend (5173): {Running/Stopped}
- Backend (8000): {Running/Stopped}

**To start the environment:**
```bash
./scripts/dev-start.sh
```

Or run:
```bash
/dev-env start
```

Waiting for environment to be ready...
```

### Step 3.2: Authentication Setup

Determine authentication requirements:

```markdown
## Authentication

**Feature requires login:** Yes / No

### If Yes:
**Test Account:**
- Email: [From project config or env]
- Password: [From project config or env]

**Auth Strategy:**
1. Check for existing session (localStorage)
2. If no session, perform login flow
3. Save session for subsequent tests
```

### Step 3.3: Database State (if needed)

Check if test data setup is required:

```markdown
## Test Data

**Requires specific data:** Yes / No

### If Yes:
- [ ] Verify test user exists
- [ ] Verify required records exist
- [ ] Note any cleanup needed after tests
```

---

## Phase 4: Test Execution

### Execution Protocol

For each test case, follow this protocol:

```bash
# === TC-{N}: {Test Name} ===

# 1. Log start
echo "Starting TC-{N}: {Test Name}"

# 2. Navigate (if needed)
agent-browser open {URL}
agent-browser wait --load networkidle

# 3. Capture "before" screenshot
agent-browser screenshot $ARGUMENTS/e2e-screenshots/tc-{N}-01-before.png

# 4. Get page state
agent-browser snapshot -i

# 5. Execute test steps
# [Series of agent-browser commands]

# 6. Capture "after" screenshot
agent-browser screenshot $ARGUMENTS/e2e-screenshots/tc-{N}-02-after.png

# 7. Verify expected result
# [Verification commands]

# 8. Log result
echo "TC-{N}: PASS/FAIL"
```

### Common Test Patterns

#### Pattern: Visual Verification
```bash
# Verify element is visible
agent-browser snapshot -i
# Check output contains expected element
# Example: Looking for "Import" button
agent-browser find text "Import" --json
# If found = PASS, if not found = FAIL
```

#### Pattern: Click and Verify
```bash
agent-browser snapshot -i
agent-browser click @{ref}
agent-browser wait 1000
agent-browser snapshot -i
# Verify expected change occurred
```

#### Pattern: Form Submission
```bash
agent-browser snapshot -i
agent-browser fill @{email-ref} "test@example.com"
agent-browser fill @{password-ref} "password123"
agent-browser screenshot $ARGUMENTS/e2e-screenshots/tc-{N}-form-filled.png
agent-browser click @{submit-ref}
agent-browser wait --load networkidle
agent-browser get url
# Verify redirect to expected page
```

#### Pattern: Navigation Flow
```bash
# Start at page A
agent-browser open {URL-A}
agent-browser wait --load networkidle
agent-browser screenshot $ARGUMENTS/e2e-screenshots/tc-{N}-page-a.png

# Navigate to page B
agent-browser click @{link-ref}
agent-browser wait --url "**{expected-path}**"
agent-browser screenshot $ARGUMENTS/e2e-screenshots/tc-{N}-page-b.png

# Verify URL
agent-browser get url
# Should contain expected path
```

#### Pattern: Modal/Dialog
```bash
agent-browser snapshot -i
agent-browser click @{trigger-ref}
agent-browser wait 500
agent-browser snapshot -i
# Verify modal content
agent-browser screenshot $ARGUMENTS/e2e-screenshots/tc-{N}-modal-open.png

# Close modal
agent-browser press Escape
agent-browser wait 500
agent-browser snapshot -i
# Verify modal closed
```

#### Pattern: Error Handling
```bash
# Trigger error condition
agent-browser fill @{input-ref} "invalid-data"
agent-browser click @{submit-ref}
agent-browser wait 1000
agent-browser snapshot -i
# Verify error message displayed
agent-browser screenshot $ARGUMENTS/e2e-screenshots/tc-{N}-error-state.png
```

### Handling Test Failures

When a test fails:

```markdown
### TC-{N} Failed

**Step Failed:** {Which step}
**Error:** {What happened}

**Debug Info:**
- URL at failure: {current URL}
- Console errors: {any console errors}
- Screenshot: tc-{N}-failure.png

**Possible Causes:**
1. [Element not found - check selectors]
2. [Timing issue - increase wait]
3. [Auth session expired - re-login]
4. [Backend error - check API]

**Recovery Action:** [Skip / Retry / Abort]
```

---

## Phase 5: Evidence Collection

### Screenshot Naming Convention

```
$ARGUMENTS/e2e-screenshots/
├── tc-001-01-before.png
├── tc-001-02-after.png
├── tc-001-03-modal.png
├── tc-002-01-login.png
├── tc-002-02-dashboard.png
└── failure-tc-003.png
```

### Console Logs

```bash
# Capture console messages
agent-browser console > $ARGUMENTS/e2e-screenshots/console-log.txt

# Capture errors specifically
agent-browser errors > $ARGUMENTS/e2e-screenshots/error-log.txt
```

### Network Requests (if relevant)

```bash
# Track API calls
agent-browser network requests --filter api > $ARGUMENTS/e2e-screenshots/api-calls.txt
```

---

## Phase 6: Report Generation

### Output File

**File**: `$ARGUMENTS/E2E-REPORT.md`

### Report Template

```markdown
# E2E Test Report: {Feature Name}

**Date:** {YYYY-MM-DD HH:MM}
**Feature:** $ARGUMENTS
**SPEC:** [./SPEC.md](./SPEC.md)
**Status:** ✅ All Passed / ⚠️ Partial / ❌ Failed

---

## Summary

| Metric | Value |
|--------|-------|
| Total Tests | {N} |
| Passed | {P} |
| Failed | {F} |
| Skipped | {S} |
| Duration | {X}m {Y}s |
| Screenshots | {N} |

### Quick Result

```
✅ TC-001: {Name} - PASSED
✅ TC-002: {Name} - PASSED
❌ TC-003: {Name} - FAILED
⏭️ TC-004: {Name} - SKIPPED
```

---

## Test Environment

| Component | Status | URL |
|-----------|--------|-----|
| Frontend | ✅ Running | http://localhost:5173 |
| Backend | ✅ Running | http://localhost:8000 |
| Auth | ✅ Logged in | {user email} |

---

## Test Results

### TC-001: {Test Name} ✅ PASSED

**Duration:** {X}s
**Type:** Visual Verification

**Steps Executed:**
1. ✅ Navigated to /dashboard/phone-numbers
2. ✅ Page loaded successfully
3. ✅ Found "Import" button
4. ✅ Found "Get Number" button

**Verification:**
- Expected: Import button visible
- Actual: Import button visible at position (x, y)

**Screenshots:**
| Step | Screenshot |
|------|------------|
| Before | ![Before](./e2e-screenshots/tc-001-01-before.png) |
| After | ![After](./e2e-screenshots/tc-001-02-after.png) |

---

### TC-002: {Test Name} ✅ PASSED

**Duration:** {X}s
**Type:** Interaction Test

**Steps Executed:**
1. ✅ Clicked Import button
2. ✅ Modal opened
3. ✅ Found "Import from GHL" option
4. ✅ Found "Import from Twilio" option

**Screenshots:**
| Step | Screenshot |
|------|------------|
| Dialog Open | ![Dialog](./e2e-screenshots/tc-002-01-dialog.png) |

---

### TC-003: {Test Name} ❌ FAILED

**Duration:** {X}s
**Type:** Workflow Test

**Steps Executed:**
1. ✅ Navigated to /dashboard/agents
2. ❌ Expected redirect to agency view

**Failure Details:**
- **Failed at step:** 2
- **Expected:** Redirect to /dashboard/agency
- **Actual:** Stayed on /dashboard/agents
- **Error:** Element not found: sub-account selector

**Debug Info:**
- Console errors: None
- Network errors: None
- Current URL: http://localhost:5173/dashboard/agents

**Screenshot:**
![Failure](./e2e-screenshots/tc-003-failure.png)

---

## Coverage Analysis

### Success Criteria Coverage

| Criterion (from SPEC) | Test | Status |
|-----------------------|------|--------|
| Phone import UI visible | TC-001, TC-002 | ✅ Covered |
| Sub-account context enforced | TC-003 | ❌ Failed |
| Limits unchanged | TC-004 | ✅ Covered |

### Untested Areas

- [ ] Error handling for invalid input
- [ ] Mobile responsive behavior
- [ ] Performance under load

---

## Screenshots Gallery

All screenshots saved to: `$ARGUMENTS/e2e-screenshots/`

| File | Description |
|------|-------------|
| tc-001-01-before.png | Phone Numbers page initial state |
| tc-001-02-after.png | Import button visible |
| tc-002-01-dialog.png | Import dialog open |
| tc-003-failure.png | Failed test state |

---

## Console Output

```
[Relevant console messages]
```

## Errors Captured

```
[Any JavaScript errors or warnings]
```

---

## Recommendations

### Issues to Address
1. **TC-003 Failed:** Sub-account redirect not working
   - Check frontend routing logic
   - Verify AuthContext tenant_type value

### Suggested Follow-ups
- [ ] Add error boundary test
- [ ] Test with slow network simulation
- [ ] Add accessibility checks

---

## Reproducing Tests

To re-run these tests manually:

```bash
# Start dev environment
./scripts/dev-start.sh

# Run specific test
agent-browser open http://localhost:5173/login
# [Follow steps from test case]
```

---

## Cleanup

After testing:
```bash
# Close browser
agent-browser close

# Screenshots are preserved in $ARGUMENTS/e2e-screenshots/
```

---

*Generated by /forge:e2e*
*FORGE: Ideate → Find → Outline → Review → Run → E2E → Guard*
```

---

## Completion Summary

After generating E2E-REPORT.md, present:

```markdown
## FORGE E2E Complete

**Report:** `$ARGUMENTS/E2E-REPORT.md`
**Screenshots:** `$ARGUMENTS/e2e-screenshots/`

### Results
| Status | Count |
|--------|-------|
| ✅ Passed | {P} |
| ❌ Failed | {F} |
| ⏭️ Skipped | {S} |

### Coverage
- {X}/{Y} success criteria verified via E2E

### Key Findings
- [Any important observations]

### Screenshots Captured
- {N} screenshots saved

### Next Steps
1. Review E2E-REPORT.md
2. Fix any failing tests
3. Run `/forge:guard $ARGUMENTS`
```

---

## Integration with forge:run

This command can be called automatically at the end of `/forge:run`:

```markdown
### Post-Implementation E2E (Optional)

After all implementation steps complete:

1. **Check if E2E needed** (see Detection triggers above)
2. **If needed, run E2E automatically:**
   ```
   /forge:e2e $ARGUMENTS
   ```
3. **Include results in completion summary**

### forge:run Completion with E2E

```markdown
## FORGE Run Complete

**Feature:** $ARGUMENTS
**Steps Completed:** [X/Y]

### Implementation Status
- [ ] All steps complete
- [ ] All validations passed

### E2E Test Status
- **Triggered:** Yes (UI changes detected)
- **Result:** ✅ 5/5 tests passed
- **Report:** [E2E-REPORT.md](./E2E-REPORT.md)
- **Screenshots:** 12 captured

### Next Step
/forge:guard $ARGUMENTS
```
```

---

## Manual Override

To force E2E tests even when auto-detection says no:

```bash
/forge:e2e $ARGUMENTS --force
```

To skip E2E even when auto-detection says yes:

```bash
/forge:e2e $ARGUMENTS --skip
```

---

## Troubleshooting

### Browser Won't Start

```bash
# Check if another browser session is running
agent-browser session list

# Kill existing sessions
agent-browser close

# Try with headed mode for debugging
agent-browser --headed open http://localhost:5173
```

### Element Not Found

```bash
# Get full page snapshot
agent-browser snapshot

# Try semantic locators
agent-browser find text "Button Text" click
agent-browser find role button click --name "Submit"
```

### Authentication Issues

```bash
# Clear existing session
agent-browser cookies clear
agent-browser storage local clear

# Re-login
agent-browser open http://localhost:5173/login
# [Login flow]
```

### Timeout Errors

```bash
# Increase wait time
agent-browser wait 5000

# Wait for specific condition
agent-browser wait --load networkidle
agent-browser wait --url "**/expected-path**"
```

---

## Best Practices

1. **Always take screenshots** at decision points
2. **Wait for network idle** after navigation
3. **Re-snapshot after interactions** that change the DOM
4. **Use semantic locators** when refs are unstable
5. **Handle errors gracefully** - capture state on failure
6. **Clean up** - close browser when done

---

*FORGE: Ideate → Find → Outline → Review → Run → E2E → Guard*
